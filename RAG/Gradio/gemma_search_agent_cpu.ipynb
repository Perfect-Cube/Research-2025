{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42ZERa6ZLhp",
        "outputId": "66ea6846-9a8a-45f5-ec78-9a6b5d1fdd9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.49.0\n",
            "    Uninstalling transformers-4.49.0:\n",
            "      Successfully uninstalled transformers-4.49.0\n",
            "Successfully installed transformers-4.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token 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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bgUcC6NZPLs",
        "outputId": "2aae648e-7fa1-4b27-a030-db0f3c766a3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `mistral` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `mistral`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cpu\")\n",
        "\n",
        "messages = [\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
        "        },\n",
        "    ],\n",
        "]\n",
        "\n",
        "output = pipe(messages, max_new_tokens=50)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-nA_Z0FZqNY",
        "outputId": "f3885e5e-12f9-4a34-9f2a-83f5a7506bda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Write a poem on Hugging Face, the company'}]}, {'role': 'assistant', 'content': \"Okay, here's a poem about Hugging Face, aiming to capture its essence and impact:\\n\\n**The Neural Web**\\n\\nA digital landscape, vast and bright,\\nHugging Face emerges, a guiding light.\\nNo single lab,\"}]}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo-search groq mcp-agent nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbanNu_kapsR",
        "outputId": "a63ef730-a25c-458f-c2cb-8010f4c98236"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-7.5.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.20.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting mcp-agent\n",
            "  Downloading mcp_agent-0.0.11-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Collecting primp>=0.14.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Collecting fastapi>=0.115.6 (from mcp-agent)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting instructor>=1.7.7 (from mcp-agent)\n",
            "  Downloading instructor-1.7.7-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mcp>=1.2.1 (from mcp-agent)\n",
            "  Downloading mcp-1.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting numpy>=2.1.3 (from mcp-agent)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-distro>=0.50b0 (from mcp-agent)\n",
            "  Downloading opentelemetry_distro-0.52b1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.29.0 (from mcp-agent)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pydantic-settings>=2.7.0 (from mcp-agent)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from mcp-agent) (6.0.2)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from mcp-agent) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from mcp-agent) (1.6.1)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from mcp-agent) (0.15.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.6->mcp-agent)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (3.11.14)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (3.1.6)\n",
            "Collecting jiter<0.9,>=0.6.1 (from instructor>=1.7.7->mcp-agent)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (1.66.5)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.7.7->mcp-agent) (9.0.0)\n",
            "Collecting httpx-sse>=0.4 (from mcp>=1.2.1->mcp-agent)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp>=1.2.1->mcp-agent)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting uvicorn>=0.23.1 (from mcp>=1.2.1->mcp-agent)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api~=1.12 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-distro>=0.50b0->mcp-agent) (1.31.0)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-distro>=0.50b0->mcp-agent)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk~=1.13 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-distro>=0.50b0->mcp-agent) (1.31.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation==0.52b1->opentelemetry-distro>=0.50b0->mcp-agent)\n",
            "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-distro>=0.50b0->mcp-agent) (24.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-distro>=0.50b0->mcp-agent) (1.17.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-semantic-conventions==0.52b1->opentelemetry-instrumentation==0.52b1->opentelemetry-distro>=0.50b0->mcp-agent) (1.2.18)\n",
            "Collecting opentelemetry-api~=1.12 (from opentelemetry-distro>=0.50b0->mcp-agent)\n",
            "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api~=1.12->opentelemetry-distro>=0.50b0->mcp-agent) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.29.0->mcp-agent) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-http>=1.29.0->mcp-agent)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-http>=1.29.0->mcp-agent)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk~=1.13 (from opentelemetry-distro>=0.50b0->mcp-agent)\n",
            "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.31.1->opentelemetry-exporter-otlp-proto-http>=1.29.0->mcp-agent) (5.29.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.7.0->mcp-agent)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->mcp-agent) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->mcp-agent) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->mcp-agent) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->mcp-agent) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->mcp-agent) (3.6.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->mcp-agent) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.7.7->mcp-agent) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor>=1.7.7->mcp-agent) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->mcp-agent) (0.1.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.52.0->instructor>=1.7.7->mcp-agent) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor>=1.7.7->mcp-agent) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor>=1.7.7->mcp-agent) (2.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api~=1.12->opentelemetry-distro>=0.50b0->mcp-agent) (3.21.0)\n",
            "Downloading duckduckgo_search-7.5.3-py3-none-any.whl (20 kB)\n",
            "Downloading groq-0.20.0-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mcp_agent-0.0.11-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.6/154.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.7.7-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mcp-1.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_distro-0.52b1-py3-none-any.whl (3.3 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.31.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, python-dotenv, primp, opentelemetry-proto, numpy, jiter, httpx-sse, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, duckduckgo-search, sse-starlette, pydantic-settings, opentelemetry-semantic-conventions, groq, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, mcp, instructor, opentelemetry-exporter-otlp-proto-http, opentelemetry-distro, mcp-agent\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.9.0\n",
            "    Uninstalling jiter-0.9.0:\n",
            "      Successfully uninstalled jiter-0.9.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.31.0\n",
            "    Uninstalling opentelemetry-api-1.31.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.31.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.52b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.52b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.31.0\n",
            "    Uninstalling opentelemetry-sdk-1.31.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed duckduckgo-search-7.5.3 fastapi-0.115.12 groq-0.20.0 httpx-sse-0.4.0 instructor-1.7.7 jiter-0.8.2 mcp-1.5.0 mcp-agent-0.0.11 numpy-2.2.4 opentelemetry-api-1.31.1 opentelemetry-distro-0.52b1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-http-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 primp-0.14.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 sse-starlette-2.2.1 starlette-0.46.1 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# A simple synchronous agent that holds tools in a dictionary.\n",
        "class SimpleAgent:\n",
        "    def __init__(self, tools: dict):\n",
        "        self.tools = tools\n",
        "\n",
        "    def call_tool(self, tool_name: str, input_str: str) -> str:\n",
        "        tool = self.tools.get(tool_name)\n",
        "        if not tool:\n",
        "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
        "        return tool(input_str)\n",
        "\n",
        "# Synchronous DuckDuckGo search tool.\n",
        "def duckduckgo_search_tool(query: str) -> str:\n",
        "    with DDGS() as ddgs:\n",
        "        results = ddgs.text(query, max_results=5)\n",
        "    formatted = \"\"\n",
        "    for res in results:\n",
        "        title = res.get(\"title\", \"No Title\")\n",
        "        url = res.get(\"href\", \"No URL\")\n",
        "        snippet = res.get(\"body\", \"No snippet available\")\n",
        "        formatted += f\"Title: {title}\\nURL: {url}\\nSnippet: {snippet}\\n\\n\"\n",
        "    return formatted.strip()\n",
        "\n",
        "# Synchronous Gemma transformers tool for text generation.\n",
        "def gemma_transformers_tool(prompt: str) -> str:\n",
        "    # Initialize the text generation pipeline with Gemma.\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"google/gemma-3-1b-it\",\n",
        "        device=\"cpu\"               # Ensure CUDA is available for GPU acceleration.   # Use bfloat16 for potential performance benefits.\n",
        "    )\n",
        "    # Generate text using the prompt.\n",
        "    output = pipe(prompt, max_new_tokens=2048)\n",
        "    # Return the generated text from the first result.\n",
        "    return output[0][\"generated_text\"]\n",
        "\n",
        "def main():\n",
        "    # Create a simple agent with two tools: one for searching and one for generating.\n",
        "    agent = SimpleAgent(tools={\n",
        "        \"search\": duckduckgo_search_tool,\n",
        "        \"generate\": gemma_transformers_tool,\n",
        "    })\n",
        "\n",
        "    print(\"Welcome to the MCP Agent!\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        user_query = input(\"Enter your query: \").strip()\n",
        "        if user_query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # Step 1: Use the search tool to get DuckDuckGo search results.\n",
        "        search_results = agent.call_tool(\"search\", user_query)\n",
        "\n",
        "        # Step 2: Build a prompt that includes the search results as context.\n",
        "        combined_prompt = (\n",
        "            \"Answer the following question using only the context provided below.\\n\\n\"\n",
        "            \"Search Results:\\n\" + search_results + \"\\n\\n\"\n",
        "            \"Question: \" + user_query + \"\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "        # Step 3: Call the Gemma transformers tool with the combined prompt.\n",
        "        final_answer = agent.call_tool(\"generate\", combined_prompt)\n",
        "\n",
        "        print(\"\\nFinal Answer:\")\n",
        "        print(final_answer)\n",
        "        print(\"\\nSources:\")\n",
        "        print(search_results)\n",
        "        print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCDi3MP6bWSZ",
        "outputId": "169bb83f-2aaa-494a-95cf-d94f00759ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the MCP Agent!\n",
            "Type 'exit' to quit.\n",
            "\n",
            "Enter your query: explain about model context protocol\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Answer:\n",
            "Answer the following question using only the context provided below.\n",
            "\n",
            "Search Results:\n",
            "Title: What is Model Context Protocol (MCP)? Complete Guide to AI's ...\n",
            "URL: https://ikala.ai/blog/ai-trends/mcp-model-context-protocol-intro_en/\n",
            "Snippet: The Model Context Protocol (MCP) is an open standard introduced by Anthropic in late 2024 that revolutionizes how AI models interact with external information sources. Think of it as a \"USB-C port for AI applications\" - a universal connector that standardizes how AI systems access and utilize external data and tools.\n",
            "\n",
            "Title: Introducing the Model Context Protocol \\ Anthropic\n",
            "URL: https://www.anthropic.com/news/model-context-protocol\n",
            "Snippet: Model Context Protocol. The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\n",
            "\n",
            "Title: MCP (Model Context Protocol): The Bridge Between AI and Tools Explained\n",
            "URL: https://blog.piax.org/mcp-model-context-protocol-the-bridge-between-ai-and-tools-explained/\n",
            "Snippet: The Model Context Protocol (MCP) creates a standardized communication layer between large language models (LLMs) and external tools/services, making it significantly easier for AI assistants to connect with and use multiple tools without complex integration challenges. This protocol represents the next evolution in AI capabilities, setting the stage for more powerful and versatile AI assistants.\n",
            "\n",
            "Title: Introduction - Model Context Protocol\n",
            "URL: https://modelcontextprotocol.io/introduction\n",
            "Snippet: MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n",
            "\n",
            "Title: What is Model Context Protocol (MCP): Explained - Composio\n",
            "URL: https://composio.dev/blog/what-is-model-context-protocol-mcp-explained/\n",
            "Snippet: They provide additional context to the LLMs. • Prompt Templates: Pre-defined templates or instructions that guide language model interactions. Tools are model-controlled, while Reosuces and Prompts are user-controlled. The models can automatically discover and invoke tools based on a given context. The \"protocol\" in Model Context Protocol\n",
            "\n",
            "Question: explain about model context protocol\n",
            "Answer:\n",
            "The Model Context Protocol (MCP) is an open standard introduced by Anthropic that enables developers to build secure, two-way connections between their data sources and AI-powered tools. It’s like a “USB-C port for AI applications” - a universal connector that standardizes how AI systems access and utilize external data and tools. It creates a standardized communication layer between large language models (LLMs) and external tools/services, making it significantly easier for AI assistants to connect with and use multiple tools without complex integration challenges.\n",
            "It's also a protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications.\n",
            "\n",
            "```\n",
            "```\n",
            "The Prompt Templates: Pre-defined templates or instructions that guide language model interactions. Tools are model-controlled, while Reosuces and Prompts are user-controlled. The models can automatically discover and invoke tools based on a given context. The \"protocol\" in Model Context Protocol.\n",
            "```\n",
            "\n",
            "\n",
            "Sources:\n",
            "Title: What is Model Context Protocol (MCP)? Complete Guide to AI's ...\n",
            "URL: https://ikala.ai/blog/ai-trends/mcp-model-context-protocol-intro_en/\n",
            "Snippet: The Model Context Protocol (MCP) is an open standard introduced by Anthropic in late 2024 that revolutionizes how AI models interact with external information sources. Think of it as a \"USB-C port for AI applications\" - a universal connector that standardizes how AI systems access and utilize external data and tools.\n",
            "\n",
            "Title: Introducing the Model Context Protocol \\ Anthropic\n",
            "URL: https://www.anthropic.com/news/model-context-protocol\n",
            "Snippet: Model Context Protocol. The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\n",
            "\n",
            "Title: MCP (Model Context Protocol): The Bridge Between AI and Tools Explained\n",
            "URL: https://blog.piax.org/mcp-model-context-protocol-the-bridge-between-ai-and-tools-explained/\n",
            "Snippet: The Model Context Protocol (MCP) creates a standardized communication layer between large language models (LLMs) and external tools/services, making it significantly easier for AI assistants to connect with and use multiple tools without complex integration challenges. This protocol represents the next evolution in AI capabilities, setting the stage for more powerful and versatile AI assistants.\n",
            "\n",
            "Title: Introduction - Model Context Protocol\n",
            "URL: https://modelcontextprotocol.io/introduction\n",
            "Snippet: MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n",
            "\n",
            "Title: What is Model Context Protocol (MCP): Explained - Composio\n",
            "URL: https://composio.dev/blog/what-is-model-context-protocol-mcp-explained/\n",
            "Snippet: They provide additional context to the LLMs. • Prompt Templates: Pre-defined templates or instructions that guide language model interactions. Tools are model-controlled, while Reosuces and Prompts are user-controlled. The models can automatically discover and invoke tools based on a given context. The \"protocol\" in Model Context Protocol\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Enter your query: good job my guy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    }
  ]
}