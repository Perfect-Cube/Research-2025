{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2e_n3q-koQz",
        "outputId": "3aa817a8-9930-4c79-c8fd-a3987a9be980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def load_model():\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    return processor, model, embedder\n",
        "\n",
        "def generate_caption(image_path, processor, model):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        caption_ids = model.generate(**inputs)\n",
        "    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def get_text_embedding(text, embedder):\n",
        "    embedding = embedder.encode(text, convert_to_tensor=True)\n",
        "    return embedding.cpu().numpy()\n",
        "\n",
        "def store_in_vector_db(embeddings, image_names):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, \"vector_db.index\")\n",
        "    with open(\"image_names.txt\", \"w\") as f:\n",
        "        for name in image_names:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "def load_vector_db():\n",
        "    index = faiss.read_index(\"vector_db.index\")\n",
        "    with open(\"image_names.txt\", \"r\") as f:\n",
        "        image_names = [line.strip() for line in f.readlines()]\n",
        "    return index, image_names\n",
        "\n",
        "def process_images(image_folder):\n",
        "    processor, model, embedder = load_model()\n",
        "    results = {}\n",
        "    image_names = []\n",
        "    embeddings_list = []\n",
        "\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            caption = generate_caption(image_path, processor, model)\n",
        "            embedding = get_text_embedding(caption, embedder)\n",
        "            results[filename] = {\n",
        "                \"caption\": caption,\n",
        "                \"embedding\": embedding\n",
        "            }\n",
        "            image_names.append(filename)\n",
        "            embeddings_list.append(embedding)\n",
        "\n",
        "    embeddings_array = np.vstack(embeddings_list)\n",
        "    store_in_vector_db(embeddings_array, image_names)\n",
        "    return results\n",
        "\n",
        "def rank_images(user_prompt):\n",
        "    _, _, embedder = load_model()  # Get all three returned values\n",
        "    index, image_names = load_vector_db()\n",
        "    query_embedding = get_text_embedding(user_prompt, embedder)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=len(image_names))\n",
        "    ranked_results = [(image_names[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
        "    ranked_results.sort(key=lambda x: x[1])  # Sort by distance (lower is better)\n",
        "\n",
        "    print(\"Ranking of images based on user prompt:\")\n",
        "    for rank, (image, score) in enumerate(ranked_results, 1):\n",
        "        print(f\"{rank}. {image} (Score: {score:.4f})\")\n",
        "\n",
        "# Example usage:\n",
        "image_folder = \"/content/sample_data\"  # Change this to your image folder path\n",
        "results = process_images(image_folder)\n",
        "\n",
        "# Example query:\n",
        "user_prompt = \"A big horse is running in the greenland\"\n",
        "rank_images(user_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdGcFP1HpMXY",
        "outputId": "40cdd660-35d8-4dc1-f096-f722fa94d097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking of images based on user prompt:\n",
            "1. Horse.jpg (Score: 0.8127)\n",
            "2. Peacock.jpg (Score: 1.5186)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main one"
      ],
      "metadata": {
        "id": "GgPmOYqH5jcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def load_model():\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    return processor, model, embedder\n",
        "\n",
        "def generate_caption(image_path, processor, model):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        caption_ids = model.generate(**inputs)\n",
        "    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def get_text_embedding(text, embedder):\n",
        "    embedding = embedder.encode(text, convert_to_tensor=True)\n",
        "    return embedding.cpu().numpy()\n",
        "\n",
        "def store_in_vector_db(embeddings, image_names):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, \"vector_db.index\")\n",
        "    with open(\"image_names.txt\", \"w\") as f:\n",
        "        for name in image_names:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "def load_vector_db():\n",
        "    index = faiss.read_index(\"vector_db.index\")\n",
        "    with open(\"image_names.txt\", \"r\") as f:\n",
        "        image_names = [line.strip() for line in f.readlines()]\n",
        "    return index, image_names\n",
        "\n",
        "def query_groq_api(user_prompt, image_captions):\n",
        "    api_url = \"https://api.groq.com/v1/chat/completions\"\n",
        "    headers = {\"Authorization\": \"Bearer gsk_f8FNmE6Iu3OuLUYE0Abb3FYUZrPOpNIU2CG0GIHcisUfHNE\", \"Content-Type\": \"application/json\"}\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Rank the following image captions based on how well they match the user prompt.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"User Prompt: {user_prompt}\\nCaptions: {json.dumps(image_captions)}\"}]\n",
        "    payload = {\"model\": \"gpt-4\", \"messages\": messages, \"temperature\": 0.5}\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def process_images(image_folder):\n",
        "    processor, model, embedder = load_model()\n",
        "    results = {}\n",
        "    image_names = []\n",
        "    embeddings_list = []\n",
        "\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            caption = generate_caption(image_path, processor, model)\n",
        "            embedding = get_text_embedding(caption, embedder)\n",
        "            results[filename] = {\n",
        "                \"caption\": caption,\n",
        "                \"embedding\": embedding\n",
        "            }\n",
        "            image_names.append(filename)\n",
        "            embeddings_list.append(embedding)\n",
        "\n",
        "    embeddings_array = np.vstack(embeddings_list)\n",
        "    store_in_vector_db(embeddings_array, image_names)\n",
        "    return results\n",
        "\n",
        "def rank_images(user_prompt):\n",
        "    _, _, embedder = load_model()  # Get all three returned values\n",
        "    index, image_names = load_vector_db()\n",
        "    query_embedding = get_text_embedding(user_prompt, embedder)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=len(image_names))\n",
        "    ranked_results = [(image_names[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
        "    ranked_results.sort(key=lambda x: x[1])  # Sort by distance (lower is better)\n",
        "\n",
        "    print(\"Ranking of images based on user prompt:\")\n",
        "    for rank, (image, score) in enumerate(ranked_results, 1):\n",
        "        print(f\"{rank}. {image} (Score: {score:.4f})\")\n",
        "\n",
        "# Example usage:\n",
        "image_folder = \"/content/sample_data\"  # Change this to your image folder path\n",
        "results = process_images(image_folder)\n",
        "\n",
        "# Example query:\n",
        "user_prompt = \"A yellow snake\"\n",
        "rank_images(user_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k2QEg2s1pYn",
        "outputId": "7001b0ee-c87a-4606-dda0-d3c123b4254a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking of images based on user prompt:\n",
            "1. image_snake_yellow.jpg (Score: 0.2222)\n",
            "2. image_snkae_green.jpg (Score: 0.6762)\n",
            "3. Peacock.jpg (Score: 1.4351)\n",
            "4. Horse.jpg (Score: 1.6648)\n",
            "5. image_bd.jpg (Score: 1.7384)\n"
          ]
        }
      ]
    }
  ]
}